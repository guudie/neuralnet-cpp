## About
This is a C++ implementation of the gradient descent algorithm, written from scratch with the Eigen library for linear algebraic calculations

`main` branch has all the minimum components to implement a simple fully connected network. It is, however, no longer in development. The workflow has been moved to `alt`

## Features
Currently, the network only supports fully-connected type of layers; more types of layers is expected to be added in the future.
Some implementations of activation functions include `linear`, `relu`, `sigmoid`, `tanh`. Some optimizers are also implemented, namely `adam`, `adagrad`, `momentum sgd`,...

## Build
A quick and easy way to build and test has not been programmed yet; however, it is expected to be added in the future